---
title: "`lmstudio-python`"
sidebar_title: "Installation"
description: "Install the Python SDK for LM Studio"
---

Page dedicated to introducing `lmstudio-python`, exaplain its capabilities, link the github, and provide a simple example of how to use it.

## How to install the Python SDK

## Loading models and making predictions

```lms_code_snippet
  variants:
    Python:
      language: python
      code: |
        import lmstudio as lm

        llm = lm.llm()
        response = llm.respond("I have a question, can you help?")

```

For code brevity, `lmstudio` may be imported under the shorter name, `lm`.

## Managing chat history

## Resource management with explicit client instances

The top level API in the Python SDK is primarily aimed at interactive use,
as well as other use cases where it is acceptable to use a single shared
client instance that remains connected until the Python interpreter is
shut down.

For use cases which require more explicit control over system resource
management (in particular, the network links between the SDK and the
LMStudio instance), SDK client instances support being used as context
managers (with network links being lazily created as they are needed,
but all explicitly terminated when the context manager exits):

```lms_code_snippet
  variants:
    Python (with scoped resources):
      language: python
      code: |
        import lmstudio

        with lmstudio.Client() as client:
          llm = client.llm.model()
          response = llm.respond("I have a question, can you help?")

```

Most of the top level convenience API functions map directly
to equivalent client level methods:

- `lm.list_downloaded_models(...)` -> `client.list_downloaded_models(...)`
- `lm.list_loaded_models(...)` -> `client.list_loaded_models(...)`
- `lm.add_temp_file(...)` -> `client.add_temp_file(...)`

There are two exceptions to this, which is that the model handle access
functions in the convenience API map to `model()` methods in the
corresponding model method namespaces:

- `lm.llm(...)` -> `client.llm.model(...)`
- `lm.embedding_model(...)` -> `client.embedding.model(...)`

These namespaces then also provide explicit methods for loading multiple
instances of the same model (typically with different load configuration
settings), as well as querying the loaded models of that type.

## Next steps

For more details on making predictions with LLMs (including streaming
individual tokens as they are generated by the model), refer to:

- [Chat Completion](/docs/api/sdk/chat-completion)
- [Text Completion](/docs/api/sdk/completion)
- [Structured Response](/docs/api/sdk/structured-response)
- [File Input](/docs/api/sdk/file-input)
- [Image Input](/docs/api/sdk/image-input)

Other sections provide more details on other interfaces (such as
tokenization, generating embedding vectors, or querying the LM Studio
server for details of downloaded and currently loaded models).

Outside this introductory section, examples are presented in a tabbed
format, showing the Python SDK's convenience and scoped resource interfaces,
along with the corresponding TypeScript SDK interface.
