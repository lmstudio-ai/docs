---
title: LM Studio as a Local LLM API Server
sidebar_title: Overview
description: Run an LLM API server on localhost with LM Studio
---

You can serve local LLMs from LM Studio's Developer tab, either on localhost or on the network.

LM Studio's APIs can be used through an [OpenAI compatibility mode](/docs/app/api/endpoints/openai), enhanced [REST API](/docs/api/rest-api/endpoints/rest), or through a client library like [lmstudio-js](/docs/api/sdk).

#### API options

- [TypeScript SDK](/docs/typescript) - `lmstudio-js`
- [Python SDK](/docs/python) - `lmstudio-python`
- [LM Studio REST API](/docs/app/api/endpoints/rest) (new, in beta)
- [OpenAI Compatibility endpoints](/docs/app/api/endpoints/openai)

<img src="/assets/docs/server.png" style="" data-caption="Load and server LLMs from LM Studio" />
