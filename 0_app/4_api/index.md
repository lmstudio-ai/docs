---
title: LM Studio as a Local LLM API Server
sidebar_title: Overview
description: Run an LLM API server on localhost with LM Studio
---

You can serve local LLMs from LM Studio's Developer tab, either on localhost or on the network.

LM Studio's APIs can be used through an [OpenAI compatibility mode](/docs/api/rest-api/openai-api), ehanced [REST API](/docs/api/rest-api/endpoints/endpoints), or through a client library like [lmstudio.js](/docs/api/sdk).

#### API options

- [OpenAI Compatibility endpoints](/docs/api/openai-api)
- [LM Studio REST API](/docs/api/rest-api/endpoints) (new, in beta)
- [TypeScript SDK](/docs/api/sdk) - `lmstudio.js`

<img src="/assets/docs/server.png" style="" data-caption="Load and server LLMs from LM Studio" />
