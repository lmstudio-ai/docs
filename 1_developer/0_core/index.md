---
title: LM Studio as a Local LLM API Server
sidebar_title: Local Server
description: Run an LLM API server on localhost with LM Studio
fullPage: true
---

You can serve local LLMs from LM Studio's Developer tab, either on localhost or on the network.

LM Studio's APIs can be used through [REST API](/docs/app/api/endpoints/rest), client libraries like [lmstudio-js](/docs/api/sdk) and [lmstudio-python](/docs/python), and [OpenAI compatibility endpoints](/docs/app/api/endpoints/openai)

#### API options

- [LM Studio REST API](/docs/app/api/endpoints/rest)
- [TypeScript SDK](/docs/typescript) - `lmstudio-js`
- [Python SDK](/docs/python) - `lmstudio-python`
- [OpenAI compatibility endpoints](/docs/app/api/endpoints/openai)

<img src="/assets/docs/server.png" style="" data-caption="Load and serve LLMs from LM Studio" />
